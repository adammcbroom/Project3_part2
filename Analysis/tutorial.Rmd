---
title: "Project 3: STAT302package Tutorial"
author: "Adam McBroom"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{STAT302package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

\addtolength{\headheight}{-.025\textheight} 
\thispagestyle{fancyplain} 
\rhead{\includegraphics[height=.1\textheight]{logo.png}}
\renewcommand{\headrulewidth}{0pt}


```{r, include = FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 4,
  collapse = TRUE,
  comment = "#>"
)
```

## Part 5: Tutorial for my_rf_cv

The fourth function in this package, `my_rf_cv`,  uses a random forest algorithm on the `my_penguins` data `palmerpenguins` package to predict `body_mass_g` with covariates `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm`. The function uses cross-validation to calculate and return the average rate of misclassification.

The call for `my_knn_cv` requires the parameter `k`, which is a numeric with the number of folds used in cross-validation. It returns a numeric with the cross-validation misclassification error from the model. 

Let's use `my_knn_cv` to compare the misclassification rates from cross-validation with 2, 5, and 10 folds. Below, we run the function 30 times each for each level of `k` and store the CV misclassification rate: 

```{r, include=FALSE}
my_penguins <- read.csv("../Data/my_penguins.csv")
my_gapminder <- read.csv("../Data/my_gapminder.csv")

source("../Code/my_rf_cv.R")
```



```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(kableExtra)
library(readr)
```

```{r}
out_mat <- matrix(NA, nrow = 30, ncol = 3)

for (k in c(2, 5, 10)) {
  for (i in 1:30) {
    for (j in 1:3){
      out_mat[i, j] <- my_rf_cv(k)
    }
  }
}

out_df <- as.data.frame(out_mat)
colnames(out_df) <- c("k = 2", "k = 5", "k = 10")
```

Using the stored output, we'll now plot the distributions of CV error over all 30 simulations for each level of k. 

```{r}
# Reformat output table for plot
out_mat2 <- matrix(NA, nrow = 90, ncol = 2)
out_df2 <- as.data.frame(out_mat2)
z <- c(out_df$`k = 2`, out_df$`k = 5`, out_df$`k = 10`)
out_df2[, 1] <- cbind(z)
out_df2[, 2] <- cbind(rep(c("2", "5", "10"), each = 30))
colnames(out_df2) <- c("MSE", "k")

my_plot <- ggplot(data = out_df2, 
                 aes(x = k, y = MSE)) +
            geom_boxplot(fill = "lightblue") +
            theme_bw(base_size = 20) +
            scale_x_discrete(limits = c("2", "5", "10")) +
            labs(title = "Distribution of CV estimated MSE by number of folds", 
                 x = "Number of folds", 
                 y = "CV estimated MSE") +
            theme(plot.title = element_text(hjust = 0.5, size = 15),
                  axis.text.x = element_text(size = 10),
                  axis.text.y = element_text(size = 10),
                  axis.title.x = element_text(size = 12.5),
                  axis.title.y = element_text(size = 12.5))
```

The boxplot for the `k = 2` simulations has a larger interquartile range and a median error rate in between those of the plots for the `k = 5` and `k = 10` simulations. The boxplot for the `k = 5` simulations has a similar interquartile range, and the lowest median error rate. The boxplot for the `k = 10` simulations has a smaller interquartile range and the highest median error rate.

Now, we'll generate a table displaying the average CV misclassification rate and the standard CV misclassification rate. 

```{r}
results_df <- cbind("Number of folds" = c(2, 5, 10),
                    "Mean of CV misclassification rate" = c(mean(out_df[, 1]), 
                                         mean(out_df[, 2]),
                                         mean(out_df[, 3])),
                    "Standard deviation of CV misclassification rate"
                    = c(sd(out_df[, 1]),
                        sd(out_df[, 2]),
                        sd(out_df[, 3])))

my_table <- kable_styling(kable(results_df))
```

The patterns that emerged in the table are similar to those we saw in the boxplots. The mean of the CV misclassification rate is lowest when 5 folds are used, and when 2 folds are used, the mean rate is slightly higher. The mean is highest when 10 folds are used. The standard deviation of the CV misclassification rate is highest when 5 folds are used, and is lower when 2 and 10 folds are used, 10 having the lowest standard deviation. It appears, then, that when 5 folds are used, the average CV misclassification rate is lower but more variable. When 10 are used, the rate is higher but more variable.

When 10 folds are used, each fold contains fewer observations than each of the other cases. The increased error rate may be due to the fact that the test data sets are smaller or that running more tests increases the likelihood of outlying values that drive up the mean. 

In addition, it stands to reason that standard deviation would rise alongside the number of folds: the each of the 10 sets of the training data includes 9 of the folds, so the ouputs of the models are likely to be highly correlated. Typically, this will increase the variance of the estimates. Since the 10-fold cross-validation case appears to have the lowest variability, however, this pattern may not hold true in this particular case. 

```{r}
ggsave("../Output/Figures/my_plot.pdf", my_plot, device = pdf)
saveRDS(my_table, "../Output/Results/my_table.rds")
write_csv(out_df, "../Output/Results/sim_results.csv")
```
